{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "alpine-provider",
   "metadata": {},
   "source": [
    "## Numerical Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "noble-intranet",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "altered-garage",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_numeric_gradient(f, x, dLdf=None, h=1e-7):\n",
    "    \n",
    "    flat_x = x.contiguous().flatten()\n",
    "    grad = torch.zeros_like(x)\n",
    "    flat_grad = grad.flatten()\n",
    "\n",
    "    # Initialize upstream gradient to be ones if not provide\n",
    "    if dLdf is None:\n",
    "        y = f(x)\n",
    "        dLdf = torch.ones_like(y)\n",
    "    dLdf = dLdf.flatten()\n",
    "\n",
    "    # iterate over all indexes in x\n",
    "    for i in range(flat_x.shape[0]):\n",
    "        oldval = flat_x[i].item()  # Store the original value\n",
    "        flat_x[i] = oldval + h  # Increment by h\n",
    "        fxph = f(x).flatten()  # Evaluate f(x + h)\n",
    "        flat_x[i] = oldval - h  # Decrement by h\n",
    "        fxmh = f(x).flatten()  # Evaluate f(x - h)\n",
    "        flat_x[i] = oldval  # Restore original value\n",
    "\n",
    "        # compute the partial derivative with centered formula\n",
    "        dfdxi = (fxph - fxmh) / (2 * h)\n",
    "\n",
    "        # use chain rule to compute dLdx\n",
    "        flat_grad[i] = dLdf.dot(dfdxi).item()\n",
    "        #flat_grad[i] = dLdf.float().dot(dfdxi).item()\n",
    "\n",
    "    # Note that since flat_grad was only a reference to grad,\n",
    "    # we can just return the object in the shape of x by returning grad\n",
    "    return grad\n",
    "\n",
    "\n",
    "def rel_error(x, y, eps=1e-10):\n",
    "\n",
    "    top = (x - y).abs().max().item()\n",
    "    bot = (x.abs() + y.abs()).clamp(min=eps).max().item()\n",
    "    return top / bot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coastal-sixth",
   "metadata": {},
   "source": [
    "# Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "manual-overview",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvFward(object):\n",
    "    def forward(x, w, b, conv_param):\n",
    "        out = None\n",
    "        num_train, Channel, H_x, W_x = x.shape\n",
    "        num_f, Channel, H_f, W_f = w.shape\n",
    "        hi = 0\n",
    "        wi = 0\n",
    "        stride = conv_param['stride']\n",
    "        pad = conv_param['pad']\n",
    "\n",
    "        H_out = 1 + (H_x + 2 * pad - H_f) // stride #2\n",
    "        W_out = 1 + (W_x + 2 * pad - W_f) // stride\n",
    "\n",
    "        p2d = (pad, pad, pad, pad) # pad last dim by (pad, pad) and 2nd to last by (pad, pad)\n",
    "        x_pad = torch.nn.functional.pad(x, p2d, \"constant\", 0)\n",
    "        out = torch.zeros(num_train, num_f, H_out, W_out).to(x.dtype)\n",
    "\n",
    "        for k in range(num_train):\n",
    "            for i in range(num_f):\n",
    "                for hi in range(H_out):\n",
    "                    step_h = hi * stride\n",
    "                    for wi in range(W_out):\n",
    "                        step_w = wi * stride\n",
    "                        sample = x_pad[k, :, step_h:(step_h+H_f), step_w:(step_w+W_f)]\n",
    "\n",
    "                        out[k, i, hi, wi] = torch.sum(sample * w[i,]) + b[i]\n",
    "\n",
    "        cache = (x, w, b, conv_param)\n",
    "        return out, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pointed-absolute",
   "metadata": {},
   "source": [
    "# Backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "backed-billion",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBprop(object):    \n",
    "    def backward(dout, cache):\n",
    "        \n",
    "        num_train, num_dout, H_dout, W_dout = dout.shape\n",
    "        x, w, b, conv_param = cache\n",
    "        num_train, Channel, H_x, W_x = x.shape         \n",
    "        dx, dw, db = None, None, None\n",
    "        dx, dw, db = torch.zeros(x.shape), torch.zeros(w.shape), torch.zeros(b.shape)\n",
    "        num_f, Channel, H_f, W_f = w.shape\n",
    "        pad = conv_param['pad']\n",
    "        stride = conv_param['stride']\n",
    "\n",
    "        p2d = (pad, pad, pad, pad) # pad last dim by (pad, pad) and 2nd to last by (pad, pad)\n",
    "        dout_pad = torch.nn.functional.pad(dout, p2d, \"constant\", 0)\n",
    "\n",
    "        x_pad = torch.nn.functional.pad(x, p2d, \"constant\", 0).to(x.dtype).to(x.device)\n",
    "        dx_pad = torch.zeros(x_pad.shape).to(x.dtype).to(x.device)\n",
    "        w_rot = w.rot90(2, [2, 3])\n",
    "        _, _, H_wrot, W_wrot = w_rot.shape\n",
    "\n",
    "        H_dw = (H_x + 2 * pad - H_dout) // stride + 1\n",
    "        W_dw = (W_x + 2 * pad - W_dout) // stride + 1\n",
    "        H_dx = (H_dout + 2 * pad - H_wrot) // stride + 1\n",
    "        W_dx = (W_dout + 2 * pad - W_wrot) // stride + 1\n",
    "       \n",
    "        for k in range(num_train):\n",
    "            for fi in range(num_f):\n",
    "                for hi in range(H_dout):\n",
    "                    step_h = hi * stride\n",
    "                    for wi in range(W_dout):\n",
    "                        step_w = wi * stride\n",
    "                        sample = x_pad[k, :, step_h:(step_h + H_f), step_w:(step_w + W_f)]\n",
    "                        dw[fi,] += sample * dout[k, fi, hi, wi]\n",
    "                        dx_pad[k, :, step_h:(step_h+H_f), step_w:(step_w+W_f)] += w[fi,] * dout[k, fi, hi, wi]\n",
    "\n",
    "        dx = dx_pad[:, :, pad:(pad+H_x), pad:(pad+W_x)]\n",
    "        \n",
    "        # another method                \n",
    "#         for k in range(num_train):\n",
    "#             for i in range(num_f):\n",
    "#                 step_h_dw, step_h_dx = 0, 0\n",
    "#                 for hi_dw in range(H_dw):\n",
    "#                     step_w_dw = 0\n",
    "#                     for wi_dw in range(W_dw):\n",
    "#                         for c in range(Channel):\n",
    "#                             sample_x = x_pad[k, c, step_h_dw:(step_h_dw + H_dout),step_w_dw:(step_w_dw + W_dout)]\n",
    "#                             # dW = X conv. dout\n",
    "#                             dw[i, c, hi_dw, wi_dw] += torch.sum(sample_x * dout[k, i,])\n",
    "#                         step_w_dw += stride\n",
    "#                     step_h_dw += stride\n",
    "#                 #print(H_dx)\n",
    "#                 for hi_dx in range(H_dx):\n",
    "#                     #print(hi_dx)\n",
    "#                     step_w_dx = 0\n",
    "#                     for wi_dx in range(W_dx):\n",
    "#                         for c in range(Channel):            \n",
    "#                             sample_dout = dout_pad[k,i,step_h_dx:(step_h_dx+H_wrot),step_w_dx:(step_w_dx+W_wrot)]\n",
    "\n",
    "#                             # dX = dout_pad conv. W_rot\n",
    "#                             dx[k, c, hi_dx, wi_dx] += torch.sum(sample_dout * w_rot[i, c,])\n",
    "#                         step_w_dx += stride\n",
    "#                     step_h_dx += stride\n",
    "                    \n",
    "        db = dout.sum(dim=3).sum(dim=2).sum(dim=0)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        return dx, dw, db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interested-schedule",
   "metadata": {},
   "source": [
    "## check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "suspected-harris",
   "metadata": {},
   "outputs": [],
   "source": [
    "# too large!!!\n",
    "# x = torch.randn(10, 3, 31, 31, dtype=torch.float64, device='cpu')\n",
    "# w = torch.randn(25, 3, 3, 3, dtype=torch.float64, device='cpu')\n",
    "# b = torch.randn(25, dtype=torch.float64, device='cpu')\n",
    "# dout = torch.randn(10, 25, 16, 16, dtype=torch.float64, device='cpu')\n",
    "# #x_cuda, w_cuda, b_cuda, dout_cuda = x.to('cuda'), w.to('cuda'), b.to('cuda'), dout.to('cuda')\n",
    "# conv_param = {'stride': 2, 'pad': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "revolutionary-finnish",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Conv.backward function\n",
      "dx error:  5.6149568876129245e-09\n",
      "dw error:  4.2332766033218214e-08\n",
      "db error:  1.3408573952983589e-10\n"
     ]
    }
   ],
   "source": [
    "dx_num = compute_numeric_gradient(lambda x: ConvFward.forward(x, w, b, conv_param)[0], x, dout)\n",
    "dw_num = compute_numeric_gradient(lambda w: ConvFward.forward(x, w, b, conv_param)[0], w, dout)\n",
    "db_num = compute_numeric_gradient(lambda b: ConvFward.forward(x, w, b, conv_param)[0], b, dout)\n",
    "\n",
    "out, cache = ConvFward.forward(x, w, b, conv_param)\n",
    "dx, dw, db = ConvBprop.backward(dout, cache)\n",
    "\n",
    "print('Testing Conv.backward function')\n",
    "print('dx error: ', rel_error(dx, dx_num))\n",
    "print('dw error: ', rel_error(dw, dw_num))\n",
    "print('db error: ', rel_error(db, db_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "early-christopher",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-0.1606, -0.5360, -0.5324, -0.3892],\n",
      "          [-0.4095, -0.6521, -0.6530, -0.2448],\n",
      "          [-0.4088, -0.6555, -0.6563, -0.2489],\n",
      "          [-0.3314, -0.1663, -0.1708,  0.1766]],\n",
      "\n",
      "         [[-0.0486, -0.4780, -0.4743, -0.4431],\n",
      "          [-0.4065, -0.6654, -0.6663, -0.2612],\n",
      "          [-0.4057, -0.6688, -0.6696, -0.2653],\n",
      "          [-0.4404, -0.2377, -0.2421,  0.2142]],\n",
      "\n",
      "         [[ 0.0634, -0.4199, -0.4163, -0.4971],\n",
      "          [-0.4034, -0.6788, -0.6796, -0.2776],\n",
      "          [-0.4027, -0.6821, -0.6829, -0.2817],\n",
      "          [-0.5493, -0.3091, -0.3135,  0.2518]]],\n",
      "\n",
      "\n",
      "        [[[-0.0272, -0.4024, -0.4007, -0.3654],\n",
      "          [-0.3586, -0.6414, -0.6417, -0.2801],\n",
      "          [-0.3630, -0.6426, -0.6429, -0.2769],\n",
      "          [-0.3144, -0.2616, -0.2636,  0.0457]],\n",
      "\n",
      "         [[-0.0614, -0.3754, -0.3737, -0.3042],\n",
      "          [-0.3761, -0.6462, -0.6465, -0.2674],\n",
      "          [-0.3804, -0.6474, -0.6477, -0.2643],\n",
      "          [-0.2977, -0.2934, -0.2954, -0.0029]],\n",
      "\n",
      "         [[-0.0957, -0.3484, -0.3467, -0.2429],\n",
      "          [-0.3935, -0.6510, -0.6513, -0.2548],\n",
      "          [-0.3979, -0.6522, -0.6525, -0.2516],\n",
      "          [-0.2809, -0.3252, -0.3272, -0.0515]]]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "x_shape = torch.tensor((2, 3, 4, 4))\n",
    "w_shape = torch.tensor((5, 3, 4, 4))\n",
    "x = torch.linspace(-0.1, 0.5, steps=torch.prod(x_shape), dtype=torch.float64, device='cpu').reshape(*x_shape)\n",
    "w = torch.linspace(-0.2, 0.3, steps=torch.prod(w_shape), dtype=torch.float64, device='cpu').reshape(*w_shape)\n",
    "b = torch.linspace(-0.1, 0.2, steps=5, dtype=torch.float64, device='cpu')\n",
    "\n",
    "conv_param = {\"stride\": 2, \"pad\": 1}\n",
    "out, cache = ConvFward.forward(x, w, b, conv_param)\n",
    "dout = torch.randn(2, 5, 2, 2, dtype=torch.float64, device='cpu')\n",
    "dx, dw, db = ConvBprop.backward(dout, cache)\n",
    "print(dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "enhanced-principle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1,2,3,4]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
